<h1>Chapter 1</h1>

<h2>1.Introduction</h2>

<p><strong>Training Set</strong> : is used to tune the <strong>parameters</strong> of an adaptive model</p>

<ul>
<li>What about adaptive model? How to select adaptive model? </li>
<li>How to design or select an good adaptive model??</li>
</ul>

<p><strong>Learning phase</strong> : The precise form of the function(adaptive model) is determined during the training phase</p>

<p><strong>General Workflow</strong>:</p>

<ol>
<li>Preprocess:
<ul>
<li>Normalization : For example : transforming different size of pics to the same one</li>
<li>Reduction : Reduce dimension or quantity, to speed up comuputation. <code>How to reduce dimension/quantity while preserving the information in those data is not easy.</code></li>
</ul></li>
<li>Training</li>
<li>Test </li>
<li>Output result</li>
</ol>

<p><strong>Classification:</strong></p>

<ul>
<li><p>supervised learning : Training data comprise examples of the input vectors <strong>along with</strong> their corresponding target vectors</p>

<ul>
<li>classification:decrete </li>
<li>regression:continuous</li>
</ul></li>
<li><p>unsupervised learning : Training data consists a set of input vector x <strong>without</strong> any corresponding target value</p>

<ul>
<li>cluster : to discover group of similar examples</li>
<li>density estimation : to determine the distribution of data within the input space.</li>
</ul></li>
<li><p>visualization : to project the data from a high-dimensional space down to two/three dimensions for the purpose of visualization. Data visualization is a hot field now. Is this can be used to preprocess data set,as mention in Reduction?</p></li>
<li><p>reinforcement learning: finding suitable actions to take in a given situation in order to maximize a reward.
Make a balance of exploration and exploitation</p>

<ul>
<li>exploration : explore the unknow space.</li>
<li>exploitation : make use of the actions that are known to yield a high reward.</li>
</ul></li>
</ul>

<p>Do you remember PSO ???</p>

<p><img src="http://latex.codecogs.com/gif.latex?V%28t&plus;1%29%20%3D%20w*V%28t%29%20&plus;%20C_%7B1%7D*R_%7B1%7D*%28P%28t%29%20-%20X%28t%29%29%20&plus;%20C_%7B2%7D*R_%7B2%7D*%28G%28t%29%20-%20X%28t%29%29" alt="Alt text" title="" /></p>

<p><img src="http://latex.codecogs.com/gif.latex?X%28t&plus;1%29%20%3D%20X%28t%29%20&plus;%20V%28t&plus;1%29" alt="text" title="" />
<!--
$$
V\(t+1\) = w\*V\(t\) + C\_{1}\*R\_{1}\*\(P\(t\) - X\(t\)\) + C\_{2}\*R\_{2}\*\(G\(t\) - X\(t\)\)
$$
$$
X(t+1) = X(t) + V(t+1)
$$
http://latex.codecogs.com/gif.latex?V(t&plus;1)&space;=&space;w*V(t)&space;&plus;&space;C_{1}*R_{1}*(P(t)&space;-&space;X(t))&space;&plus;&space;C_{2}*R_{2}*(G(t)&space;-&space;X(t))
--></p>

<p>It also need a balance between <strong>exploitation</strong> and <strong>exploration</strong>. But it has a global attraction when particle explore the unknown space. It is lucky...</p>

<p><strong>Deep Learning</strong> : a new branch of machine learning. <a href="http://en.wikipedia.org/wiki/Deep_learning">wikipedia</a></p>

<h2>2.Bayes' theorem</h2>

<p><img src="http://latex.codecogs.com/gif.latex?p%5C%28w%7CD%5C%29%20%3D%20%5Cfrac%7Bp%5C%28D%7Cw%5C%29p%5C%28w%5C%29%7D%7Bp%5C%28D%5C%29%7D" alt="Bayes theorem" title="" /></p>

<p>In that example,Bayes' theorem was used to <em>convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data.</em> 
<strong>We can adopt a similar approach when making inferences about quantities such as the parameters w in the polynomial curve fitting example.</strong></p>

<ul>
<li>We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w). </li>
<li>The <strong>effect</strong> of the observed data D = {t1 , . . . , tN } is expressed through the <strong>conditional probability p(D|w)</strong>, and we shall see later how this can be represented explicitly.</li>
</ul>

<p>In the Bayesian (or epistemological) interpretation, probability measures <em>a degree of belief</em>.</p>

<h2>Pros. &amp; Cons. of Bayes (p23.)</h2>

<p><strong>Pros.</strong></p>

<ul>
<li>1) <strong>The inclusion of prior knowledge in Bayes theorem (Formula 1.12) arises naturally.
<em>* This *</em>means</strong> Bayes theorem coincides with the cognition of people to the world. 
<em>Ex. After one observes a result, he/she will improve his/her assumption.</em></li>
</ul>

<p><strong>Cons.</strong></p>

<ul>
<li>1) <strong>The selection of the prior distribution.</strong> 
Bayesian methods based on poor choices of prior can give poor results with high confidence.</li>
</ul>

<script type="text/javascript" src="http://benweet.github.io/stackedit/lib/MathJax/MathJax.js?config=TeX-AMS_HTML"></script>
